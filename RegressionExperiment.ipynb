import numpy as np
from matplotlib import pyplot as plt
from sklearn.datasets import load_svmlight_file
from sklearn.model_selection import train_test_split
import random
import math
# 定义逻辑回归函数
def sigmoid(x,w):
    return 1 / (1 + math.exp(-np.dot(x, w)[0]))


def get_data():
    x, y  = load_svmlight_file('a9a.txt')
    x_train = np.reshape(x.todense().data, (x.shape[0], x.shape[1]))
    y_train = np.reshape(y.data, (y.shape[0], 1))
    y_train = (y_train + 1) / 2
    x_, y_ = load_svmlight_file('a9a.t')
    x_test = np.reshape(x_.todense().data, (x_.shape[0], x_.shape[1]))
    x_test = np.column_stack((x_test, np.zeros(x_.shape[0])))
    y_test = np.reshape(y_.data, (y_.shape[0], 1))
    y_test = (y_test + 1) / 2
    return x_train, y_train, x_test, y_test

def compute_loss(w, x, y):
    loss =0.0
    m = len(x)
    for i in range(m):
        y_ = sigmoid(x[i], w)
        loss += y[i] * math.log(y_) + (1 - y[i]) * math.log(1 - y_)
    return -loss / m

def compute_grad(x_,y_,w):
    grad = 0.0
    for i in range(len(x_)):
        grad += (sigmoid(x_, w)-y_[i]) * x_[i]
        if i > 1999:
            break
    return grad / 2000

def update_w_NAG(w, grad, v, mu=0.9, eta=0.001):
    v_ = v
    v = mu * v - eta * grad
    w += ((1+mu)*v - v_ * mu).reshape((123, 1))
    return w, v

def update_w_RMSProp(w, grad, G, r=0.9, eps=1e-5, eta=0.01):
    G = r * G + (1 - r) * (grad ** 2)
    w +=(-eta * grad / (np.sqrt(G )+eps)).reshape((123, 1))
    return w, G

def update_w_AdaDelta(w, grad, G, delta_t, r=0.95, eps=1e-6):
    G = r * G + (1 - r) * (grad ** 2)
    delta_theta = - np.sqrt(delta_t + eps) / np.sqrt(G + eps) * grad
    w = w + delta_theta.reshape((123,1))
    delta_t = r * delta_t + (1 - r) * (delta_theta ** 2)
    return w, G, delta_t

def update_w_Adam(w, grad, m, t, G, beta=0.9, r=0.999, eta=0.01, eps=1e-8):
    m = beta * m + (1 - beta) * grad
    G = r * G + (1 - r) * (grad ** 2)
    mt = m / (1 - beta ** t)
    #alpha = eta * np.sqrt(1 - r ** t) / (1 - beta ** t)
    vt = G / (1 - r ** t)
    w += (-eta * mt / (np.sqrt(vt) + eps)).reshape((123, 1))
    return w, m, G

def plotLossPerTime(max_cycle, nag_loss, rms_loss, adad_loss, adam_loss):
    plt.xlabel('iteration times')
    plt.ylabel('loss')
    plt.title('Logistic Regression')
    n_cycles = range(1,max_cycle+1)
    plt.plot(n_cycles, nag_loss, label="Loss of NAG", linewidth=2)
    plt.plot(n_cycles, rms_loss, label="Loss of RMSProp", linewidth=2)
    plt.plot(n_cycles, adad_loss, label="Loss of AdaDelta", linewidth=2)
    plt.plot(n_cycles, adam_loss, label="Loss of Adam", linewidth=2)
    plt.legend(loc=0)
    plt.grid()
    plt.show()


def logistic_regression(max_cycle, x_train, y_train, x_test, y_test):
    nag_w = np.zeros((x_train.shape[1], 1))
    rmsprop_w = np.zeros((x_train.shape[1], 1))
    adadelta_w = np.zeros((x_train.shape[1], 1))
    adam_w = np.zeros((x_train.shape[1], 1))

    nag_v = np.zeros(x_train.shape[1])
    rmsprop_G = np.zeros(x_train.shape[1])
    adadelta_G = np.zeros(x_train.shape[1])
    adadelta_t = np.zeros(x_train.shape[1])
    adam_m = np.zeros(x_train.shape[1])
    adam_G = np.zeros(x_train.shape[1])

    nag_loss = []
    rmsprop_loss = []
    adadelta_loss = []
    adam_loss = []
    for i in range(1, max_cycle):
        randIndex = list(range(len(x_train)))
        random.shuffle(randIndex)
        # NAG update parameters
        nag_grad = compute_grad(x_train[randIndex], y_train[randIndex], nag_w)
        nag_w, nag_v = update_w_NAG(nag_w, nag_grad, nag_v)
        nag_loss.append(compute_loss(nag_w,x_test, y_test))

        # RMSprop update parameters
        rmsprop_grad = compute_grad(x_train[randIndex], y_train[randIndex], rmsprop_w)
        rmsprop_w, rmsprop_G = update_w_RMSProp(rmsprop_w, rmsprop_grad, rmsprop_G)
        rmsprop_loss.append(compute_loss( rmsprop_w,x_test, y_test))


        # AdaDelta update parameters
        adadelta_grad = compute_grad(x_train[randIndex], y_train[randIndex], adadelta_w)
        adadelta_w, adadelta_G, adadelta_t = update_w_AdaDelta(adadelta_w, adadelta_grad, adadelta_G, adadelta_t)
        adadelta_loss.append(compute_loss(adadelta_w, x_test, y_test))

        # Adam update parameters
        adam_grad = compute_grad(x_train[randIndex], y_train[randIndex], adam_w)
        adam_w, adam_m, adam_G =update_w_Adam(adam_w, adam_grad, adam_m, i, adam_G)
        adam_loss.append(compute_loss(adam_w, x_test, y_test))
        print(i)
    return nag_loss, rmsprop_loss, adadelta_loss,  adam_loss

# main
x_train, y_train, x_test, y_test = get_data()
nag_loss, rmsprop_loss, adadelta_loss,  adam_loss = logistic_regression(101,x_train,y_train,x_test, y_test)

plotLossPerTime(100,nag_loss,rmsprop_loss,adadelta_loss,adam_loss)


